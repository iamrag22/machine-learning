{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#numpy imports\n",
    "import numpy as np\n",
    "\n",
    "# opencv imports\n",
    "import cv2\n",
    "\n",
    "# python imports\n",
    "import os\n",
    "from collections import OrderedDict, namedtuple\n",
    "from itertools import product\n",
    "\n",
    "# progress bar - tqdm\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images from msft repo for cats and dogs\n",
    "CATS = \"PetImages\\Cat\"\n",
    "DOGS = \"PetImages\\Dog\"\n",
    "\n",
    "CATEGORIES = {\n",
    "    CATS : 0,\n",
    "    DOGS : 1\n",
    "}\n",
    "\n",
    "OUTPUT_FILE = \"cats_and_dogs.npy\"\n",
    "IMAGE_SIZE = 50\n",
    "\n",
    "# Prepare dataset and store in .npy file\n",
    "\n",
    "\n",
    "results = []\n",
    "input_image_processing_failed = 0\n",
    "cat_images = 0\n",
    "dog_images = 0\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    for file_name in tqdm(os.listdir(category)):\n",
    "        if \"jpg\" in file_name:\n",
    "            try:\n",
    "                file_path = os.path.join(category, file_name)\n",
    "                image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "                resize_image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "                \n",
    "                # Get the category value for 1 hot encoding\n",
    "                label_index = CATEGORIES[category]\n",
    "                \n",
    "                # [1,0] for cat and [0,1] for dog\n",
    "                label  = np.eye(2)[label_index]\n",
    "                \n",
    "                # Each entry is (np.array(image), np.array(label)) tuple\n",
    "                result = (np.array(resize_image), label)\n",
    "                results.append(result)\n",
    "                \n",
    "                if category == CATS:\n",
    "                    cat_images+= 1\n",
    "                elif category == DOGS:\n",
    "                    dog_images+=1\n",
    "            \n",
    "            except Exception as e:\n",
    "                input_image_processing_failed+=1\n",
    "\n",
    "print(\"Cat Images: \", cat_images, \" Dog Images: \", dog_images, \" Failed Images: \",  input_image_processing_failed)\n",
    "\n",
    "np.random.shuffle(results)\n",
    "np.save(OUTPUT_FILE, results, allow_pickle=True)\n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OUTPUT_FILE = \"cats_and_dogs.npy\"\n",
    "IMAGE_SIZE = 50\n",
    "\n",
    "TEST_PERCENTAGE = 0.1\n",
    "\n",
    "# Build tensors for training and test dataset\n",
    "\n",
    "def get_train_test_dataset_tensors():\n",
    "    data = np.load(OUTPUT_FILE, allow_pickle=True)\n",
    "    # print(data.shape) # (24946, 2)\n",
    "    # The 2 corresponds to size of tuple (images_np_array, labels_np_array)\n",
    "\n",
    "\n",
    "    split_index = int(0.1*data.shape[0])\n",
    "    \n",
    "    \n",
    "    images_tensor = torch.tensor([entry[0] for entry in data], dtype= torch.float)\n",
    "    images_tensor = images_tensor/255.0\n",
    "    images_tensor = images_tensor.to(device)\n",
    "    \n",
    "    labels_tensor = torch.tensor([entry[1] for entry in data], dtype= torch.float)\n",
    "    labels_tensor = labels_tensor.to(device)\n",
    "    \n",
    "    # Here 1 corresponds to color channels\n",
    "    train_images_tensor = images_tensor[:-split_index].reshape(-1,1,IMAGE_SIZE,IMAGE_SIZE)\n",
    "    train_labels_tensor = labels_tensor[:-split_index]\n",
    "    \n",
    "    test_images_tensor = images_tensor[-split_index:].reshape(-1,1,IMAGE_SIZE,IMAGE_SIZE)\n",
    "    test_labels_tensor = labels_tensor[-split_index:]\n",
    "    \n",
    "    \n",
    "    return [(train_images_tensor, train_labels_tensor), (test_images_tensor, test_labels_tensor)]\n",
    "\n",
    "train_dataset, test_dataset = get_train_test_dataset_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Verification of data and shapes\n",
    "train_images, train_labels = train_dataset\n",
    "#print(train_images.shape, train_labels.shape)\n",
    "\n",
    "test_images, test_labels = test_dataset\n",
    "#print(test_images.shape, test_labels.shape, test_images[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatsAndDogsNetwork(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out): Linear(in_features=512, out_features=2, bias=True)\n",
      ") 512\n"
     ]
    }
   ],
   "source": [
    "# Build Network\n",
    "\n",
    "class CatsAndDogsNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5)\n",
    "        \n",
    "        self.linear_size=None\n",
    "        X = torch.rand(IMAGE_SIZE, IMAGE_SIZE).reshape(-1,1,IMAGE_SIZE,IMAGE_SIZE)\n",
    "        self.convs(X)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features = self.linear_size, out_features = 512)\n",
    "        self.out =  nn.Linear(in_features = 512, out_features = 2)\n",
    "    \n",
    "    def convs(self, X):\n",
    "        X =  F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X, kernel_size=2, stride=2)\n",
    "        \n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, kernel_size=2, stride=2)\n",
    "        \n",
    "        X = F.relu(self.conv3(X))\n",
    "        X = F.max_pool2d(X, kernel_size=2, stride=2)\n",
    "        \n",
    "        # flatten from dim1. dim0 is batch size and we dont want to flatten on that\n",
    "        X = X. flatten(1,-1)\n",
    "        \n",
    "        if self.linear_size is None:\n",
    "            self.linear_size = X.shape[1]\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.convs(X)\n",
    "        \n",
    "        # FC1\n",
    "        X = F.relu(self.fc1(X))\n",
    "        \n",
    "        X = self.out(X)\n",
    "        \n",
    "        return F.softmax(X, dim=1)\n",
    "\n",
    "network = CatsAndDogsNetwork()\n",
    "network.to(device)\n",
    "print(network, network.linear_size)            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunGenerator class \n",
    "# To try various hyper parameters for the network\n",
    "class RunGenerator:\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "        runs = []\n",
    "        hyperparams = params.keys()\n",
    "        hyperparams_values = params.values()\n",
    "        \n",
    "        Run = namedtuple(\"Run\", hyperparams)\n",
    "        \n",
    "        for combination in product(*hyperparams_values):\n",
    "            run = Run(*combination)\n",
    "            runs.append(run)\n",
    "        \n",
    "        return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Manager to manage Run/Epoch level operations and logging to TensorBoard\n",
    "class RunManager():\n",
    "    def __init__(self):\n",
    "        self.run =  None\n",
    "        self.network = None\n",
    "        self.tb = None\n",
    "        \n",
    "        self.train_dataset_size = None\n",
    "        self.test_dataset_size = None\n",
    "        \n",
    "        self.epoch_id = 0\n",
    "        self.correct_predictions = 0\n",
    "        self.total_loss=0.0\n",
    "        \n",
    "    def run_start(self, run, network, train_dataset_size, test_dataset_size):\n",
    "        self.run = run\n",
    "        self.tb = SummaryWriter(comment=f\"-{run}\")\n",
    "        self.network = network\n",
    "        \n",
    "        self.train_dataset_size = train_dataset_size\n",
    "        self.test_dataset_size = test_dataset_size\n",
    "        \n",
    "        self.epoch_id = 0\n",
    "    \n",
    "    def run_end(self):\n",
    "        self.tb.close()\n",
    "    \n",
    "    def epoch_start(self):\n",
    "        self.correct_predictions = 0\n",
    "        self.total_loss=0.0\n",
    "        \n",
    "    \n",
    "    def epoch_end(self):\n",
    "        accuracy = 100*(self.correct_predictions/self.train_dataset_size)\n",
    "        \n",
    "        self.tb.add_scalar(\"Training Accuracy\", accuracy,self.epoch_id)\n",
    "        self.tb.add_scalar(\"Training Loss\", self.total_loss, self.epoch_id)\n",
    "        \n",
    "        for name, param in self.network.named_parameters():\n",
    "            self.tb.add_histogram(f\"{name}.grad\", param.grad, self.epoch_id)\n",
    "        \n",
    "        self.total_loss=0.0\n",
    "        self.epoch_id+=1\n",
    "        \n",
    "    # This will be called per batch\n",
    "    def track_correct_predictions(self, predictions, actual):\n",
    "        correct_predictions = self.get_correct_predictions(predictions, actual)\n",
    "        self.correct_predictions+= correct_predictions\n",
    "    \n",
    "    # This will be called per batch\n",
    "    def track_total_loss(self,loss):\n",
    "        self.total_loss+=loss\n",
    "    \n",
    "    def get_correct_predictions(self, predictions, actual):\n",
    "        # dim 0 is for batch. Hence we take dim=1 for labels\n",
    "        predictions = predictions.argmax(dim=1)\n",
    "        actual = actual.argmax(dim=1)\n",
    "        \n",
    "        matches = predictions.eq(actual).sum().item()\n",
    "        \n",
    "        return matches\n",
    "    \n",
    "    def record_validation_stats(self, accuracy, loss, epoch):\n",
    "        self.tb.add_scalar(\"Validation Accuracy: \", accuracy, epoch)\n",
    "        self.tb.add_scalar(\"Validation Loss: \", loss, epoch)\n",
    "        \n",
    "rm = RunManager()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_size = test_images.shape[0]\n",
    "\n",
    "def get_random_batch_of_training_data(size=32):\n",
    "    random_index = np.random.randint(size, test_images_size+1)\n",
    "    \n",
    "    train_images_batch = train_images[random_index-size:random_index]\n",
    "    train_labels_batch = train_labels[random_index-size:random_index]\n",
    "        \n",
    "    return (train_images_batch, train_labels_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [00:11<00:00, 19.06it/s]\n",
      "  1%|██                                                                                                                                                    | 3/225 [00:00<00:09, 23.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurrrrr 46.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [00:11<00:00, 20.11it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [00:11<00:00, 20.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurrrrr 53.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [00:11<00:00, 20.20it/s]\n",
      "  1%|██                                                                                                                                                    | 3/225 [00:00<00:08, 25.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurrrrr 48.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [00:11<00:00, 20.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [00:11<00:00, 20.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurrrrr 60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:08<00:00,  2.79it/s]\n",
      "  4%|██████▌                                                                                                                                                | 1/23 [00:00<00:02,  8.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurrrrr 57.99999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:08<00:00,  2.79it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:08<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurrrrr 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:08<00:00,  2.79it/s]\n",
      "  4%|██████▌                                                                                                                                                | 1/23 [00:00<00:02,  8.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurrrrr 48.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:08<00:00,  2.79it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:08<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurrrrr 51.0\n"
     ]
    }
   ],
   "source": [
    "# Generate various runs experimentation and hyoerparam tuning\n",
    "params = OrderedDict(\n",
    "    batch_size= [100, 1000],\n",
    "    lr = [0.001, 0.01]\n",
    ")\n",
    "\n",
    "runs = RunGenerator.get_runs(params)\n",
    "\n",
    "# Initialize Run Manager\n",
    "rm = RunManager()\n",
    "\n",
    "train_size = train_images.shape[0]\n",
    "test_size = test_images.shape[0]\n",
    "\n",
    "#print(train_images.shape, test_images.shape, train_size, test_size)\n",
    "\n",
    "# # Training Loop\n",
    "EPOCHS = 3\n",
    "\n",
    "for run in runs:\n",
    "    optimizer = optim.Adam(network.parameters(), run.lr)\n",
    "    loss_func = nn.MSELoss()\n",
    "    rm.run_start(run, network, train_size, test_size)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        rm.epoch_start()\n",
    "        for i in tqdm(range(0,train_size, run.batch_size)):\n",
    "            train_images_batch = train_images[i:i+run.batch_size]\n",
    "            train_labels_batch = train_labels[i:i+run.batch_size]\n",
    "            \n",
    "            network.zero_grad()\n",
    "            \n",
    "            predicted = network(train_images_batch)\n",
    "            loss = loss_func(predicted, train_labels_batch)\n",
    "            \n",
    "            rm.track_correct_predictions(predicted, train_labels_batch)\n",
    "            rm.track_total_loss(loss.item())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "        rm.epoch_end()\n",
    "        \n",
    "        if epoch %2 == 0:\n",
    "            # Test on a random batch of some size\n",
    "            random_batch_images, random_batch_labels = get_random_batch_of_training_data(100)\n",
    "            with torch.no_grad():\n",
    "                predicted = network(random_batch_images)\n",
    "                correct_predictions = rm.get_correct_predictions(predicted, random_batch_labels)\n",
    "                accuracy = 100*(correct_predictions/100)\n",
    "                print(\"Accurrrrr\", accuracy)\n",
    "                loss = loss_func(predicted, random_batch_labels)\n",
    "                rm.record_validation_stats(accuracy,loss, epoch)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Test on entire test dataset\n",
    "        predicted = network(test_images)\n",
    "        correct_predictions = rm.get_correct_predictions(predicted, test_labels)\n",
    "        accuracy = 100*(correct_predictions/test_size)\n",
    "        loss = loss_func(predicted, test_labels)\n",
    "        rm.record_validation_stats(accuracy,loss, epoch)\n",
    "\n",
    "    \n",
    "    rm.run_end()\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('pytorchenv': conda)",
   "language": "python",
   "name": "python38264bitpytorchenvcondaa1d2a31854e0434a85965068b8c72b15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
