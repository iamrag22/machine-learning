{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1:Prepare the environment\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "actions = env.action_space.n\n",
    "states = env.observation_space.n\n",
    "env.reset()\n",
    "actions, states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the q table\n",
    "table = np.zeros((states, actions))\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 : Hyperparams\n",
    "gamma = 0.99\n",
    "learning_rate = 0.1 \n",
    "\n",
    "# Exploration rate - This will be updated during training\n",
    "explorate_start = 1\n",
    "explorate_end = 0.01\n",
    "explorate_decay = 0.001\n",
    "exploration = 1\n",
    "\n",
    "max_episodes = 10000\n",
    "max_steps_episode = 100\n",
    "\n",
    "rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    episode_reward = 0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    for step in range(max_steps_episode):\n",
    "        # Choose an action based on explore or exploit\n",
    "        if exploration > random.random():\n",
    "            # Explore\n",
    "            action = random.randrange(actions)\n",
    "            \n",
    "        else:\n",
    "            # exploit using existing information\n",
    "            # Find the action in the current state which has the max q value\n",
    "            action = np.argmax(table[state, :])\n",
    "        \n",
    "        # Take action\n",
    "        new_state, reward, done, info  = env.step(action)\n",
    "        episode_reward+=new_state\n",
    "        \n",
    "        # Update table[state, action]\n",
    "        # The optimal q value is the max of all [new_state, action] values in the new state.\n",
    "        # Note that this is not the np.argmax() which is the action index\n",
    "        optimal_q_value = np.max(table[new_state, :])\n",
    "        table[state, action] =  (1-learning_rate)*table[state, action] + learning_rate*(reward + gamma*optimal_q_value)\n",
    "        \n",
    "        # Update the state\n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            rewards.append(episode_reward)\n",
    "            break\n",
    "    \n",
    "    # Update exploration after each episode\n",
    "    exploration = explorate_end + (explorate_start-explorate_end)*(np.exp(-explorate_decay*episode))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards per 100 episodes 23.23\n",
      "Average rewards per 100 episodes 22.21\n",
      "Average rewards per 100 episodes 26.91\n",
      "Average rewards per 100 episodes 31.66\n",
      "Average rewards per 100 episodes 33.61\n",
      "Average rewards per 100 episodes 35.13\n",
      "Average rewards per 100 episodes 44.31\n",
      "Average rewards per 100 episodes 46.96\n",
      "Average rewards per 100 episodes 45.93\n",
      "Average rewards per 100 episodes 72.06\n",
      "Average rewards per 100 episodes 61.51\n",
      "Average rewards per 100 episodes 71.72\n",
      "Average rewards per 100 episodes 70.1\n",
      "Average rewards per 100 episodes 91.65\n",
      "Average rewards per 100 episodes 85.5\n",
      "Average rewards per 100 episodes 95.24\n",
      "Average rewards per 100 episodes 107.6\n",
      "Average rewards per 100 episodes 105.71\n",
      "Average rewards per 100 episodes 108.35\n",
      "Average rewards per 100 episodes 125.91\n",
      "Average rewards per 100 episodes 110.54\n",
      "Average rewards per 100 episodes 125.06\n",
      "Average rewards per 100 episodes 117.78\n",
      "Average rewards per 100 episodes 155.26\n",
      "Average rewards per 100 episodes 178.16\n",
      "Average rewards per 100 episodes 140.07\n",
      "Average rewards per 100 episodes 162.05\n",
      "Average rewards per 100 episodes 184.87\n",
      "Average rewards per 100 episodes 168.28\n",
      "Average rewards per 100 episodes 166.49\n",
      "Average rewards per 100 episodes 190.26\n",
      "Average rewards per 100 episodes 193.55\n",
      "Average rewards per 100 episodes 203.13\n",
      "Average rewards per 100 episodes 174.99\n",
      "Average rewards per 100 episodes 204.95\n",
      "Average rewards per 100 episodes 196.79\n",
      "Average rewards per 100 episodes 194.8\n",
      "Average rewards per 100 episodes 198.83\n",
      "Average rewards per 100 episodes 190.31\n",
      "Average rewards per 100 episodes 198.12\n",
      "Average rewards per 100 episodes 168.88\n",
      "Average rewards per 100 episodes 214.79\n",
      "Average rewards per 100 episodes 205.28\n",
      "Average rewards per 100 episodes 195.73\n",
      "Average rewards per 100 episodes 214.06\n",
      "Average rewards per 100 episodes 203.05\n",
      "Average rewards per 100 episodes 233.43\n",
      "Average rewards per 100 episodes 211.47\n",
      "Average rewards per 100 episodes 202.11\n",
      "Average rewards per 100 episodes 209.65\n",
      "Average rewards per 100 episodes 215.97\n",
      "Average rewards per 100 episodes 203.45\n",
      "Average rewards per 100 episodes 209.38\n",
      "Average rewards per 100 episodes 239.76\n",
      "Average rewards per 100 episodes 236.43\n",
      "Average rewards per 100 episodes 219.81\n",
      "Average rewards per 100 episodes 198.31\n",
      "Average rewards per 100 episodes 226.36\n",
      "Average rewards per 100 episodes 226.76\n",
      "Average rewards per 100 episodes 228.49\n",
      "Average rewards per 100 episodes 219.03\n",
      "Average rewards per 100 episodes 227.05\n",
      "Average rewards per 100 episodes 235.35\n",
      "Average rewards per 100 episodes 231.96\n",
      "Average rewards per 100 episodes 225.88\n",
      "Average rewards per 100 episodes 190.87\n",
      "Average rewards per 100 episodes 235.59\n",
      "Average rewards per 100 episodes 207.33\n",
      "Average rewards per 100 episodes 224.0\n",
      "Average rewards per 100 episodes 232.11\n",
      "Average rewards per 100 episodes 253.05\n",
      "Average rewards per 100 episodes 236.59\n",
      "Average rewards per 100 episodes 239.71\n",
      "Average rewards per 100 episodes 245.71\n",
      "Average rewards per 100 episodes 210.94\n",
      "Average rewards per 100 episodes 205.71\n",
      "Average rewards per 100 episodes 237.91\n",
      "Average rewards per 100 episodes 222.39\n",
      "Average rewards per 100 episodes 207.38\n",
      "Average rewards per 100 episodes 216.36\n",
      "Average rewards per 100 episodes 238.66\n",
      "Average rewards per 100 episodes 249.86\n",
      "Average rewards per 100 episodes 237.75\n",
      "Average rewards per 100 episodes 227.75\n",
      "Average rewards per 100 episodes 209.76\n",
      "Average rewards per 100 episodes 231.71\n",
      "Average rewards per 100 episodes 224.9\n",
      "Average rewards per 100 episodes 250.76\n",
      "Average rewards per 100 episodes 231.81\n",
      "Average rewards per 100 episodes 221.45\n",
      "Average rewards per 100 episodes 223.74\n",
      "Average rewards per 100 episodes 227.5\n",
      "Average rewards per 100 episodes 249.81\n",
      "Average rewards per 100 episodes 234.68\n",
      "Average rewards per 100 episodes 227.18\n",
      "Average rewards per 100 episodes 220.76\n",
      "Average rewards per 100 episodes 219.6\n",
      "Average rewards per 100 episodes 216.71\n",
      "Average rewards per 100 episodes 225.29\n",
      "Average rewards per 100 episodes 244.43\n"
     ]
    }
   ],
   "source": [
    "# Sanity check on rewards\n",
    "\n",
    "for i in range(0, len(rewards), 100):\n",
    "    print(\"Average rewards per 100 episodes\", np.average(rewards[i:i+100]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        # Best action\n",
    "        action = np.argmax(table[state, :])\n",
    "        \n",
    "        # Apply action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        state = new_state\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"Won\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"Lost\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit401b29ecbd81407abd908ab7b486b90a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
